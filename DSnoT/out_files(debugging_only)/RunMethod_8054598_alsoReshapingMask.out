============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
torch 1.13.1
transformers 4.28.0
accelerate 0.18.0
# of gpus:  1
Model path: haoranxu/ALMA-7B
Model type: None
source and target lang: cs, en
model type: llama
loading llm model haoranxu/ALMA-7B
use device  cuda:0
pruning starts
Loading calibration data...
Available configs:  ['cs-en']
Loading dataset with source_lang=cs and target_lang=en
Loaded dataset. Sample entry {'translation': {'cs': 'Osmadvacetiletý šéfkuchař nalezen mrtev v obchodě v San Francisku', 'en': '28-Year-Old Chef Found Dead at San Francisco Mall'}}
valdata features: {'translation': {'cs': Value(dtype='string', id=None), 'en': Value(dtype='string', id=None)}}
translation
dataset loading complete

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:07,  3.67s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:03,  3.62s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.17s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.29s/it]
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /gpfs/home3/scur1762/DSnoT/main.py:143 in <module>                           │
│                                                                              │
│   140 │   │   tokenizer.save_pretrained(args.save_model)                     │
│   141                                                                        │
│   142 if __name__ == '__main__':                                             │
│ ❱ 143 │   main()                                                             │
│   144                                                                        │
│                                                                              │
│ /gpfs/home3/scur1762/DSnoT/main.py:101 in main                               │
│                                                                              │
│    98 │   │   print("pruning starts")                                        │
│    99 │   │   if args.model_type == "llama":                                 │
│   100 │   │   │   if args.prune_method == "wanda":                           │
│ ❱ 101 │   │   │   │   prune_wanda(args, model, tokenizer, device, prune_n=pr │
│   102 │   │   │   elif args.prune_method == "magnitude":                     │
│   103 │   │   │   │   prune_magnitude(args, model, tokenizer, device, prune_ │
│   104 │   │   │   elif args.prune_method == "sparsegpt":                     │
│                                                                              │
│ /gpfs/home3/scur1762/DSnoT/lib/prune.py:231 in prune_wanda                   │
│                                                                              │
│    228 │   print("dataset loading complete")                                 │
│    229 │                                                                     │
│    230 │   with torch.no_grad():                                             │
│ ❱  231 │   │   inps, outs, attention_mask, position_ids = prepare_calibratio │
│    232 │   │   │   args, model, dataloader, device                           │
│    233 │   │   )                                                             │
│    234                                                                       │
│                                                                              │
│ /gpfs/home3/scur1762/DSnoT/lib/prune.py:116 in prepare_calibration_input     │
│                                                                              │
│    113 │   layers[0] = Catcher(layers[0])                                    │
│    114 │   for batch in dataloader:                                          │
│    115 │   │   try:                                                          │
│ ❱  116 │   │   │   model(batch[0].to(device))                                │
│    117 │   │   except ValueError:                                            │
│    118 │   │   │   pass                                                      │
│    119 │   layers[0] = layers[0].module                                      │
│                                                                              │
│ /home/scur1762/.conda/envs/DSnoT/lib/python3.8/site-packages/torch/nn/module │
│ s/module.py:1194 in _call_impl                                               │
│                                                                              │
│   1191 │   │   # this function, and just call forward.                       │
│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │
│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │
│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │
│   1195 │   │   # Do not call functions when jit is used                      │
│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │
│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │
│                                                                              │
│ /home/scur1762/.conda/envs/DSnoT/lib/python3.8/site-packages/accelerate/hook │
│ s.py:165 in new_forward                                                      │
│                                                                              │
│   162 │   │   │   with torch.no_grad():                                      │
│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │
│   164 │   │   else:                                                          │
│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │
│   166 │   │   return module._hf_hook.post_forward(module, output)            │
│   167 │                                                                      │
│   168 │   module.forward = new_forward                                       │
│                                                                              │
│ /home/scur1762/.conda/envs/DSnoT/lib/python3.8/site-packages/transformers/mo │
│ dels/llama/modeling_llama.py:687 in forward                                  │
│                                                                              │
│   684 │   │   return_dict = return_dict if return_dict is not None else self │
│   685 │   │                                                                  │
│   686 │   │   # decoder outputs consists of (dec_features, layer_state, dec_ │
│ ❱ 687 │   │   outputs = self.model(                                          │
│   688 │   │   │   input_ids=input_ids,                                       │
│   689 │   │   │   attention_mask=attention_mask,                             │
│   690 │   │   │   position_ids=position_ids,                                 │
│                                                                              │
│ /home/scur1762/.conda/envs/DSnoT/lib/python3.8/site-packages/torch/nn/module │
│ s/module.py:1194 in _call_impl                                               │
│                                                                              │
│   1191 │   │   # this function, and just call forward.                       │
│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │
│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │
│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │
│   1195 │   │   # Do not call functions when jit is used                      │
│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │
│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │
│                                                                              │
│ /home/scur1762/.conda/envs/DSnoT/lib/python3.8/site-packages/accelerate/hook │
│ s.py:165 in new_forward                                                      │
│                                                                              │
│   162 │   │   │   with torch.no_grad():                                      │
│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │
│   164 │   │   else:                                                          │
│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │
│   166 │   │   return module._hf_hook.post_forward(module, output)            │
│   167 │                                                                      │
│   168 │   module.forward = new_forward                                       │
│                                                                              │
│ /home/scur1762/.conda/envs/DSnoT/lib/python3.8/site-packages/transformers/mo │
│ dels/llama/modeling_llama.py:577 in forward                                  │
│                                                                              │
│   574 │   │   │   │   │   None,                                              │
│   575 │   │   │   │   )                                                      │
│   576 │   │   │   else:                                                      │
│ ❱ 577 │   │   │   │   layer_outputs = decoder_layer(                         │
│   578 │   │   │   │   │   hidden_states,                                     │
│   579 │   │   │   │   │   attention_mask=attention_mask,                     │
│   580 │   │   │   │   │   position_ids=position_ids,                         │
│                                                                              │
│ /home/scur1762/.conda/envs/DSnoT/lib/python3.8/site-packages/torch/nn/module │
│ s/module.py:1194 in _call_impl                                               │
│                                                                              │
│   1191 │   │   # this function, and just call forward.                       │
│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │
│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │
│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │
│   1195 │   │   # Do not call functions when jit is used                      │
│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │
│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │
│                                                                              │
│ /gpfs/home3/scur1762/DSnoT/lib/prune.py:97 in forward                        │
│                                                                              │
│     94 │   │   │   if attention_mask.size(-1) < model.seqlen:                │
│     95 │   │   │   │   attention_mask = torch.nn.functional.pad(attention_ma │
│     96 │   │   │   # Reshape attention_mask to the expected size (1, 1, 2048 │
│ ❱   97 │   │   │   attention_mask = attention_mask.unsqueeze(1).expand(-1, 1 │
│     98 │   │   │                                                             │
│     99 │   │   │   position_ids = kwargs["position_ids"]                     │
│    100 │   │   │   if position_ids.size(1) < model.seqlen:                   │
╰──────────────────────────────────────────────────────────────────────────────╯
RuntimeError: expand(torch.cuda.HalfTensor{[1, 1, 1, 46, 2048]}, size=[-1, 1, 
2048, 2048]): the number of sizes provided (4) must be greater or equal to the 
number of dimensions in the tensor (5)
srun: error: gcn22: task 0: Exited with exit code 1
srun: Terminating StepId=8054598.0
